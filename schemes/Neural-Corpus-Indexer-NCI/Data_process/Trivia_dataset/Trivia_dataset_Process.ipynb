{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f41a853",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import numpy as np\n",
    "from tqdm import trange, tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bbc76c",
   "metadata": {},
   "source": [
    "###### Download TriviaQA dataset from https://nlp.cs.washington.edu/triviaqa/data/triviaqa-rc.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d03965",
   "metadata": {},
   "outputs": [],
   "source": [
    "evidence_dir = os.path.join(\"evidence/wikipedia\")\n",
    "qa_train_path = os.path.join(\"qa/wikipedia-train.json\")\n",
    "qa_dev_path = os.path.join(\"qa/wikipedia-dev.json\")\n",
    "qa_test_path = os.path.join(\"qa/wikipedia-test-without-answers.json\")\n",
    "\n",
    "def txt2title(text):\n",
    "    return os.path.splitext(text)[0].replace('_', ' ')\n",
    "\n",
    "full_doc = {}\n",
    "global_index = 0\n",
    "for file_name in os.listdir(evidence_dir):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        title = txt2title(file_name)\n",
    "        assert title not in full_doc, f\"dup title for {file_name}\"\n",
    "        with open(os.path.join(evidence_dir, file_name)) as f:\n",
    "            body = f.read().replace('\\n', ' ').replace('\\t', ' ').replace('\\r', ' ')[:10240]\n",
    "        full_doc[file_name] = (str(global_index), title, body)\n",
    "        global_index += 1\n",
    "lines = ['\\t'.join(v) + '\\n' for v in full_doc.values()]\n",
    "with open(\"trivia_qa_fulldoc.csv\", \"w\") as f:\n",
    "    f.writelines(lines)\n",
    "\n",
    "\n",
    "for in_file, out_file in zip((qa_train_path, qa_dev_path, qa_test_path), (\"trivia_qa_train.csv\", \"trivia_qa_dev.csv\", \"trivia_qa_test.csv\")):\n",
    "    with open(in_file) as f:\n",
    "        raw = json.load(f)\n",
    "\n",
    "    lines = []\n",
    "    for item in raw[\"Data\"]:\n",
    "        query = item[\"Question\"]\n",
    "        doc_ids = [full_doc[doc[\"Filename\"]][0] for doc in item[\"EntityPages\"]]\n",
    "        lines.append(query + \"\\t\" + ','.join(doc_ids) + \"\\n\")\n",
    "\n",
    "    with open(out_file, \"w\") as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbd69b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def lower(x):\n",
    "    try:\n",
    "        text = tokenizer.tokenize(x)[:512]\n",
    "        id_ = tokenizer.convert_tokens_to_ids(text)\n",
    "        return tokenizer.decode(id_)\n",
    "    except:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba114a0f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('trivia_qa_train.csv',\n",
    "                        names=[\"query\", \"docid\"],\n",
    "                        encoding='utf-8', header=None, sep='\\t')\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4482cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_val = pd.read_csv('trivia_qa_dev.csv',\n",
    "                        names=[\"query\", \"docid\"],\n",
    "                        encoding='utf-8', header=None, sep='\\t').loc[:, ['query', 'docid']]\n",
    "df_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad5e1d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('trivia_qa_test.csv',\n",
    "                        names=[\"query\", \"docid\"],\n",
    "                        encoding='utf-8', header=None, sep='\\t').loc[:, ['query', 'docid']]\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv('trivia_qa_fulldoc.csv',\n",
    "                         names=[\"docid\", \"title\", \"content\"],\n",
    "                        encoding='utf-8', header=None, sep='\\t')\n",
    "df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19d2f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full['title'] = df_full['title'].map(lower)\n",
    "df_drop_title = df_full.drop_duplicates('title').reset_index(drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f0c8bb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_drop_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d51a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "title_doc_id = {}\n",
    "for i in trange(len(df_drop_title)):\n",
    "    title_doc_id[df_drop_title['title'][i]] = i\n",
    "\n",
    "origin_new_id = {}\n",
    "for i in trange(len(df_full)):\n",
    "    origin_new_id[df_full['docid'][i]] = title_doc_id[df_full['title'][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740ca274",
   "metadata": {},
   "outputs": [],
   "source": [
    "## doc pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca419aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e35d84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "file_pool = open(\"Trivia_doc_content.tsv\", 'w') \n",
    "\n",
    "for i in trange(len(df_drop_title)):\n",
    "    file_pool.write('\\t'.join([str(df_drop_title['docid'][i]), str(origin_new_id[df_drop_title['docid'][i]]), str(df_drop_title['title'][i]), str(df_drop_title['content'][i]), str(df_drop_title['title'][i]) + str(df_drop_title['content'][i])]) + '\\n')\n",
    "    file_pool.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e986362b",
   "metadata": {},
   "source": [
    "## Generate BERT embeddings for each document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64579c77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Execute the following command to get bert embedding pkl file\n",
    "## Use 4 GPU\n",
    "!./bert/Trivia_bert.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0671f5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "output_bert_base_tensor = []\n",
    "output_bert_base_id_tensor = []\n",
    "for num in trange(4):\n",
    "    with open(f'bert/pkl/Trivia_output_tensor_512_content_{num}.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_tensor.extend(data)\n",
    "\n",
    "    with open(f'bert/pkl/Trivia_output_tensor_512_content_{num}_id.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_id_tensor.extend(data)\n",
    "\n",
    "\n",
    "train_file = open(f\"bert/Trivia_doc_content_embedding_bert_512.tsv\", 'w') \n",
    "\n",
    "for idx, doc_tensor in enumerate(output_bert_base_tensor):\n",
    "    embedding = '|'.join([str(elem) for elem in doc_tensor])\n",
    "    train_file.write('\\t'.join([str(output_bert_base_id_tensor[idx]), '', '', '', '', '', 'en', embedding]) + '\\n')\n",
    "    train_file.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367c05a",
   "metadata": {},
   "source": [
    "## Apply Hierarchical K-Means on it to generate semantic IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f815d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Execute the following command to get kmeans id of the documents\n",
    "!./kmeans/kmeans_Trivia.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae25738",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kmeans/IDMapping_Trivia_bert_512_k30_c30_seed_7.pkl', 'rb') as f:\n",
    "    kmeans_trivia_doc_dict = pickle.load(f)\n",
    "## random id : newid\n",
    "new_kmeans_trivia_doc_dict_512 = {}\n",
    "for old_docid in kmeans_trivia_doc_dict.keys():\n",
    "    new_kmeans_trivia_doc_dict_512[str(old_docid)] = '-'.join(str(elem) for elem in kmeans_trivia_doc_dict[old_docid])\n",
    "\n",
    "new_kmeans_trivia_doc_dict_512_int_key = {}\n",
    "for key in new_kmeans_trivia_doc_dict_512:\n",
    "    new_kmeans_trivia_doc_dict_512_int_key[int(key)] = new_kmeans_trivia_doc_dict_512[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f03704",
   "metadata": {},
   "source": [
    "## Query Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f695b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Execute the following command to generate queries for the documents\n",
    "## Use 4 GPU\n",
    "!./qg/Trivia_qg.sh 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbe1072",
   "metadata": {},
   "outputs": [],
   "source": [
    "## merge parallel results\n",
    "output_bert_base_tensor_qg = []\n",
    "output_bert_base_id_tensor_qg = []\n",
    "for num in trange(4):\n",
    "    with open(f'qg/pkl/Trivia_output_tensor_512_content_64_15_{num}.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_tensor_qg.extend(data)\n",
    "\n",
    "    with open(f'qg/pkl/Trivia_output_tensor_512_content_64_15_{num}_id.pkl', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    f.close()\n",
    "    output_bert_base_id_tensor_qg.extend(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086a2735",
   "metadata": {},
   "outputs": [],
   "source": [
    "qg_dict = {}\n",
    "for i in trange(len(output_bert_base_tensor_qg)):\n",
    "    if(output_bert_base_id_tensor_qg[i] not in qg_dict):\n",
    "        qg_dict[output_bert_base_id_tensor_qg[i]] = [output_bert_base_tensor_qg[i]]\n",
    "    else:\n",
    "        qg_dict[output_bert_base_id_tensor_qg[i]].append(output_bert_base_tensor_qg[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e716ea",
   "metadata": {},
   "source": [
    "## Genarate training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32da935",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_query_docid = {}\n",
    "for i in trange(len(df_train)):\n",
    "    if(len(df_train['query'][i].split('\\n')) == 1):\n",
    "        train_query_docid[df_train['query'][i]] = [int(elem) for elem in df_train['docid'][i].split(',')]\n",
    "\n",
    "file_train = open(\"train.tsv\", 'w')\n",
    "\n",
    "count = 0\n",
    "for query in tqdm(train_query_docid.keys()):\n",
    "    for i in range(len(train_query_docid[query])):\n",
    "        id_ori = train_query_docid[query][i]\n",
    "        new_id = origin_new_id[id_ori]\n",
    "        file_train.write('\\t'.join([query, str(id_ori), str(new_id), new_kmeans_trivia_doc_dict_512_int_key[int(new_id)]]) + '\\n')\n",
    "        file_train.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f2db8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_query_docid = {}\n",
    "for i in trange(len(df_val)):\n",
    "    if(len(df_val['query'][i].split('\\n')) == 1):\n",
    "        val_query_docid[df_val['query'][i]] = [int(elem) for elem in df_val['docid'][i].split(',')]\n",
    "\n",
    "file_val = open(\"dev.tsv\", 'w')\n",
    "\n",
    "count = 0\n",
    "for query in tqdm(val_query_docid.keys()):\n",
    "    id_ori_ = []\n",
    "    new_id_ = []\n",
    "    kmeans_ = []\n",
    "    for i in range(len(val_query_docid[query])):\n",
    "        id_ori = str(val_query_docid[query][i])\n",
    "        new_id = str(origin_new_id[int(id_ori)])\n",
    "        id_ori_.append(id_ori)\n",
    "        new_id_.append(new_id)\n",
    "        kmeans_.append(new_kmeans_trivia_doc_dict_512_int_key[int(new_id)])\n",
    "    \n",
    "    id_ori_ = ','.join(id_ori_)\n",
    "    new_id_ = ','.join(new_id_)\n",
    "    kmeans_ = ','.join(kmeans_)\n",
    "    \n",
    "    file_val.write('\\t'.join([query, str(id_ori_), str(new_id_), kmeans_]) + '\\n')\n",
    "    file_val.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377de8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_query_docid = {}\n",
    "for i in trange(len(df_test)):\n",
    "    if(len(df_test['query'][i].split('\\n')) == 1):\n",
    "        test_query_docid[df_val['query'][i]] = [int(elem) for elem in df_test['docid'][i].split(',')]\n",
    "\n",
    "file_test = open(\"test.tsv\", 'w')\n",
    "\n",
    "count = 0\n",
    "for query in tqdm(test_query_docid.keys()):\n",
    "    id_ori_ = []\n",
    "    new_id_ = []\n",
    "    kmeans_ = []\n",
    "    for i in range(len(test_query_docid[query])):\n",
    "        id_ori = str(test_query_docid[query][i])\n",
    "        new_id = str(origin_new_id[int(id_ori)])\n",
    "        id_ori_.append(id_ori)\n",
    "        new_id_.append(new_id)\n",
    "        kmeans_.append(new_kmeans_trivia_doc_dict_512_int_key[int(new_id)])\n",
    "    \n",
    "    id_ori_ = ','.join(id_ori_)\n",
    "    new_id_ = ','.join(new_id_)\n",
    "    kmeans_ = ','.join(kmeans_)\n",
    "    \n",
    "    file_test.write('\\t'.join([query, str(id_ori_), str(new_id_), kmeans_]) + '\\n')\n",
    "    file_test.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b532b48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "QG_NUM = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b92402",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "qg_file = open(\"trivia_512_qg.tsv\", 'w') \n",
    "\n",
    "for queryid in tqdm(qg_dict):\n",
    "    for query in qg_dict[queryid][:QG_NUM]:\n",
    "        qg_file.write('\\t'.join([query, queryid, new_kmeans_trivia_doc_dict_512_int_key[int(queryid)]]) + '\\n')\n",
    "        qg_file.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76cef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_title['new_id'] = df_drop_title['docid'].map(origin_new_id)\n",
    "\n",
    "df_drop_title['kmeas_id'] = df_drop_title['new_id'].map(new_kmeans_trivia_doc_dict_512_int_key)\n",
    "\n",
    "\n",
    "df_drop_title['tc'] = df_drop_title['title'] + ' ' + df_drop_title['content']\n",
    "\n",
    "df_drop_title_ = df_drop_title.loc[:, ['tc', 'docid', 'new_id', 'kmeas_id']]  \n",
    "\n",
    "df_drop_title_.to_csv('trivia_title_cont.tsv', sep='\\t', header=None, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04795876",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_drop_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b12cc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "queryid_oldid_dict = {}\n",
    "bertid_oldid_dict = {}\n",
    "map_file = \"trivia_title_cont.tsv\"\n",
    "with open(map_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        query, queryid, oldid, bert_k30_c30 = line.split(\"\\t\")\n",
    "        queryid_oldid_dict[oldid] = queryid\n",
    "        bertid_oldid_dict[oldid] = bert_k30_c30\n",
    "\n",
    "train_file = \"Trivia_doc_content.tsv\"\n",
    "doc_aug_file = open(\"trivia_doc_aug.tsv\", 'w') \n",
    "with open(train_file, 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        _, docid, _, _, content = line.split(\"\\t\")\n",
    "        content = content.split(' ')\n",
    "        add_num = max(0, len(content)-3000) / 3000\n",
    "        for i in range(10+int(add_num)):\n",
    "            begin = random.randrange(0, len(content))\n",
    "            # if begin >= (len(content)-64):\n",
    "            #     begin = max(0, len(content)-64)\n",
    "            end = begin + 64 if len(content) > begin + 64 else len(content)\n",
    "            doc_aug = content[begin:end]\n",
    "            doc_aug = ' '.join(doc_aug).replace('\\n', ' ')\n",
    "            queryid = queryid_oldid_dict[docid]\n",
    "            bert_k30_c30 = bertid_oldid_dict[docid]\n",
    "            # doc_aug_file.write('\\t'.join([doc_aug, str(queryid), str(docid), str(bert_k30_c30)]) + '\\n')\n",
    "            doc_aug_file.write('\\t'.join([doc_aug, str(queryid), str(docid), str(bert_k30_c30)]))\n",
    "            doc_aug_file.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "faiss",
   "language": "python",
   "name": "faiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
